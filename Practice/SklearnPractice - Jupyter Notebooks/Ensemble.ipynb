{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9523809523809523\n",
      "0.9523809523809523\n",
      "0.9619047619047618\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "# ensemble means \"take mean\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "\n",
    "data_i, targeti = load_iris(return_X_y=True)\n",
    "data_train, data_test, target_train, target_test = train_test_split(data_i, targeti, random_state=42,\n",
    "                                                                    stratify=targeti, test_size=0.7)\n",
    "\n",
    "bagging = BaggingClassifier(max_samples=0.5,  # half rows\n",
    "                            max_features=0.5,  # half columns\n",
    "                            base_estimator=KNeighborsClassifier())\n",
    "\n",
    "# better than Bagging: can define how many estimators, rather than unknown number of estimators\n",
    "# worse than Bagging: cannot define the base_estimator\n",
    "rndForest = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "exTree = ExtraTreesClassifier(max_samples=0.5,  # half rows\n",
    "                              max_features=0.5,  # half columns\n",
    "                              n_estimators=10)\n",
    "\n",
    "# Basically the same\n",
    "bagging.fit(data_train, target_train)\n",
    "rndForest.fit(data_train, target_train)\n",
    "exTree.fit(data_train, target_train)\n",
    "\n",
    "print(cross_val_score(bagging, data_test, target_test, cv=5).mean())\n",
    "print(cross_val_score(rndForest, data_test, target_test, cv=5).mean())\n",
    "print(cross_val_score(exTree, data_test, target_test, cv=5).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 2 0 0 0 2 1 1 1 1 1 0 2 1 0 2 1 0 0 0 0 2 2 1 1 1 1 0 2 1 2 1 2 0 1 1\n",
      " 0 2 2 2 0 2 1 0 1 0 0 0 1 0 0 2 2 2 0 2 1 0 2 2 0 2 2 2 2 1 2 0 2 1 2 0 0\n",
      " 2 1 0 1 0 2 2 1 1 2 1 0 1 1 1 0 0 2 1 2 1 2 1 2 1 0 0 1 0 1 1]\n",
      "[0 0 2 0 0 0 2 1 1 1 1 1 0 2 1 0 2 1 0 0 0 0 2 2 1 1 1 1 0 2 1 2 1 1 0 1 1\n",
      " 0 2 2 2 0 2 1 0 1 0 0 0 1 0 0 2 1 2 0 2 1 0 2 2 0 2 2 2 2 1 2 0 2 1 2 0 0\n",
      " 2 1 0 1 0 2 2 1 1 2 1 0 1 1 1 0 0 2 1 2 2 2 1 2 1 0 0 1 0 1 1]\n",
      "[0 0 2 0 0 0 2 1 1 1 1 1 0 2 1 0 2 1 0 0 0 0 2 2 1 1 2 1 0 2 1 2 1 1 0 1 1\n",
      " 0 2 2 2 0 2 1 0 1 0 0 0 1 0 0 2 1 2 0 2 1 0 2 2 0 2 2 2 2 1 2 0 2 1 2 0 0\n",
      " 2 1 0 1 0 2 2 1 1 2 1 0 1 1 1 0 0 2 1 2 2 2 1 2 1 0 0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_predict(bagging, data_test, target_test, cv=5))\n",
    "print(cross_val_predict(rndForest, data_test, target_test, cv=5))\n",
    "print(cross_val_predict(exTree, data_test, target_test, cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9714285714285714\n",
      "0.9523809523809523\n",
      "[0 0 2 0 0 0 2 1 1 1 1 1 0 2 1 0 2 1 0 0 0 0 2 2 1 1 2 1 0 2 1 2 1 1 0 2 1\n",
      " 0 1 2 2 0 2 1 0 1 0 0 0 1 0 0 2 2 2 0 2 1 0 2 2 0 2 2 2 2 1 2 0 2 2 2 0 0\n",
      " 2 1 0 1 0 2 2 1 1 2 1 0 1 1 1 0 0 2 1 2 2 2 1 2 1 0 0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(n_estimators=100)  # strengthen weakness\n",
    "ada_clf.fit(data_train, target_train)\n",
    "\n",
    "print(ada_clf.score(data_test, target_test))\n",
    "print(cross_val_score(ada_clf, data_test, target_test, cv=5).mean())\n",
    "print(cross_val_predict(ada_clf, data_test, target_test, cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9428571428571428\n",
      "0.9523809523809523\n",
      "[0 0 2 0 0 0 2 1 1 1 1 1 0 2 1 0 2 1 0 0 0 0 2 2 1 1 2 1 0 2 1 2 1 1 0 1 1\n",
      " 0 2 2 2 0 2 1 0 1 0 0 0 1 0 0 2 1 2 0 2 1 0 2 2 0 2 2 2 2 1 2 0 2 1 2 0 0\n",
      " 2 1 0 1 0 2 2 1 1 2 1 0 1 1 1 0 0 2 1 2 2 2 1 2 1 0 0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "\n",
    "gbc = GradientBoostingClassifier(n_estimators=100)\n",
    "gbc.fit(data_train, target_train)\n",
    "\n",
    "print(gbc.score(data_test, target_test))\n",
    "print(cross_val_score(gbc, data_test, target_test, cv=5).mean())\n",
    "print(cross_val_predict(gbc, data_test, target_test, cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9278549928225018\n",
      "0.9251851432530074\n",
      "[-1.21625101e-04 -1.21625101e-04  2.02461465e+00 -1.21625101e-04\n",
      " -1.21625101e-04 -1.21625101e-04  1.99825322e+00  1.08891462e+00\n",
      "  8.22063846e-01  9.82651125e-01  9.84849620e-01  9.29685159e-01\n",
      " -1.21625101e-04  2.00143557e+00  9.72966777e-01 -1.21625101e-04\n",
      "  1.71339666e+00  9.99053070e-01 -1.21625101e-04 -1.21625101e-04\n",
      " -1.21625101e-04 -4.23738530e-04  1.94885833e+00  2.03346285e+00\n",
      "  1.01180741e+00  1.00741305e+00  1.79850294e+00  1.00240761e+00\n",
      " -4.23738530e-04  1.94396564e+00  1.68211090e+00  2.00189718e+00\n",
      "  1.00747087e+00  9.99429148e-01  5.23189001e-04  1.68350937e+00\n",
      "  9.90413789e-01 -4.23738530e-04  1.93903375e+00  1.99823734e+00\n",
      "  2.00213187e+00 -4.23738530e-04  1.99700378e+00  9.98702974e-01\n",
      "  2.80764727e-04  1.00381518e+00  2.80764727e-04  2.80764727e-04\n",
      "  2.80764727e-04  9.69247481e-01  2.80764727e-04  1.09882778e-03\n",
      "  1.99924267e+00  1.01051826e+00  1.99784143e+00  2.80764727e-04\n",
      "  2.00021800e+00  9.98992152e-01  1.42530170e-03  2.00079286e+00\n",
      "  1.99807467e+00  2.80764727e-04  2.00028882e+00  1.99720000e+00\n",
      "  1.99825841e+00  1.99824080e+00  1.01015546e+00  1.98724581e+00\n",
      "  1.71132952e-04  2.00229999e+00  1.75506757e+00  2.00034111e+00\n",
      " -1.17065272e-03  1.71132952e-04  1.99169247e+00  9.99358784e-01\n",
      "  1.71132952e-04  9.99747634e-01 -1.17065272e-03  1.99762064e+00\n",
      "  2.00411713e+00  1.00865577e+00  9.48849382e-01  2.00677553e+00\n",
      "  9.85809768e-01  4.09961203e-05  9.99511273e-01  9.99511273e-01\n",
      "  1.03581487e+00  2.56763036e-03  4.09961203e-05  1.98711887e+00\n",
      "  1.23526840e+00  2.02576020e+00  2.08450633e+00  2.03648542e+00\n",
      "  1.16136546e+00  2.00442042e+00  1.00226689e+00  4.09961203e-05\n",
      "  1.39262105e-02  1.17708645e+00  4.09961203e-05  9.99457002e-01\n",
      "  1.00320367e+00]\n"
     ]
    }
   ],
   "source": [
    "gbr = GradientBoostingRegressor(n_estimators=100)  # it is a decision tree, so it could be un-stable?\n",
    "gbr.fit(data_train, target_train)\n",
    "\n",
    "print(gbr.score(data_test, target_test))\n",
    "print(cross_val_score(gbr, data_test, target_test, cv=5).mean())\n",
    "print(cross_val_predict(gbr, data_test, target_test, cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9047619047619048\n",
      "0.9619047619047618\n",
      "[0 0 2 0 0 0 2 1 1 1 1 1 0 2 1 0 2 1 0 0 0 0 2 2 1 1 2 1 0 2 1 2 1 1 0 1 1\n",
      " 0 2 2 2 0 2 1 0 1 0 0 0 1 0 0 2 2 2 0 2 1 0 2 2 0 2 2 2 2 1 2 0 2 2 2 0 0\n",
      " 2 1 0 1 0 2 2 1 1 2 1 0 1 1 1 0 0 2 1 2 2 2 1 2 1 0 0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "\n",
    "vc = VotingClassifier(\n",
    "    estimators=[(\"someone\", rndForest), (\"bagging\", bagging), (\"LogisticRegression\", LogisticRegression()),\n",
    "                (\"SVC\", svm.SVC())],  # like Pipeline\n",
    "    # hard vote: 少数服从多数， 如果平票，那按字母排列选第一个\n",
    "    # soft vote：take average\n",
    "    voting=\"hard\",\n",
    "    weights=[2, 1, 2, 1])\n",
    "\n",
    "vc.fit(data_train, target_train)\n",
    "\n",
    "print(vc.score(data_test, target_test))\n",
    "print(cross_val_score(vc, data_test, target_test, cv=5).mean())\n",
    "print(cross_val_predict(vc, data_test, target_test, cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
